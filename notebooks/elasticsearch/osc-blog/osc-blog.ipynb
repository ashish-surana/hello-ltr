{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download corpus, judgments, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/blog.jsonl already exists\n",
      "data/osc_judgments.txt already exists\n"
     ]
    }
   ],
   "source": [
    "from ltr import download\n",
    "corpus='http://es-learn-to-rank.labs.o19s.com/blog.jsonl'\n",
    "judgments='http://es-learn-to-rank.labs.o19s.com/osc_judgments.txt'\n",
    "\n",
    "download([corpus, judgments], dest='data/');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse out OSC's blog into `articles`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': \"Lets Stop Saying 'Cognitive Search'\",\n",
       " 'url': 'https://opensourceconnections.com/blog/2019/05/28/lets-stop-saying-cognitive-search/',\n",
       " 'author': 'doug-turnbull',\n",
       " 'content': ' I consume a lot of search materials: blogs, webinars, papers, and marketing collateral. There’s a consistent theme that crops up over the years: buzzwords! I understand why this happens, and it’s not all negative. We want to invite others outside our community into what we do. Heck I write my own marketing collateral, so I get the urge to jump on a buzzword bandwagon from time-to-time. \\n\\n That being said, I want to tell you a dirty little secret. Nobody really knows what ‘cognitive search’ means in any concrete sense. Sit two people down, and ask them what problem ‘cognitive search’ solves, and you’ll get two different answers. Most likely they imagine some kind of silver-bullet solution to a unique, painful search relevance problem they’re experiencing. Problems that require careful, deep thought where there truly isn’t an easy, one-size-fits-all, silver-bullet solution. As this inevitably leads to disappointment, buzzwords like this ultimately disenchant those brought into the community. Let’s go over why it’s important to scrub buzzwords from our language if we want to help each other solve our relevance problems. \\n\\n 1. The ML techniques applied to search are extremely varied \\n\\n ‘Cognitive search’ implies a single machine learning technique. But there’s such a broad range of potential machine learning ‘lego bricks’ that just saying ‘cognitive search’ doesn’t concretely convey how a problem is solved. You might as well say the problem is solved with ‘computers.’ It could mean learning to rank to optimize relevance ranking. It might mean embeddings or knowledge graphs to improve semantic understanding of queries and content. Maybe they use contextual bandits or neural language models? \\n\\n As many Haystack talks on these subjects attest many of these solutions can just as easily create a disaster as they might be a perfect fit for the problem. The takeaway is if someone says they’re going to apply ‘cognitive search’ to a problem, ask them to get specific about the techniques they use and why they’ll be appropriate to your problems. \\n\\n 2. Expect customization, not turn-key solutions \\n\\n Search relevance requirements have subtly unique requirements that are hard for users to consciously express. An internal enterprise search engine has not much in common with Google. Even within domains, one job search experience (ie general purpose Careerbuilder) might have little resemblance to another’s (ie hourly-wage oriented Snag). None of which look anything like E-Commerce Search or Patent Examiner Search. \\n\\n Matching the machine learning lego pieces to the subtle, hard to capture specifics of an application is hard. You should expect a lot of customization. I’ve noticed that search engine product companies have A LOT of professional services. If you care about the problem, you’ll either need a competent internal team or a long-term implementation relationship with a vendor. An honest question is whether you would rather own that customization yourself? Or outsource it to an external entity? If getting it right is core value, tread carefully. Don’t assume because it’s all just ‘cognitive search’ that you don’t need to think very deeply about the problem. \\n\\n 3. ‘Cognitive Search’ is a hypothesis, not a solution. Can you measure it? \\n\\n The silver bullet to improving search quality is not machine learning, its measurement and experimental methodology. With so many possible ways of applying machine learning to relevance, you need a system for evaluating which will work, and which won’t. It’s not as simple as getting a ‘cognitive search’ engine and saying its done and dusted. Every proposed solution should be approached as a hypothesis, not a guarantee. \\n\\n Can your organization scientifically study how a hypothesis impacts your business? Do you have KPIs tied to business value, that demonstrate search relevance’s impact on your business? Do you have a way to systematically measure what search results were good/bad/indifferent for a given search query? Have you broken this down by persona, and user segments? \\n\\n In any progression of a search team, this is the first and best use of data scientists. It’s the foundation for everything else the team does. It’s hard work, but an extremely worthwhile way to increase your pace of innovation with real, concrete machine learning methods. \\n\\n No more buzzwords, let’s talk about the real techniques used \\n\\n Instead of talking about vague buzzwords like ‘cognitive search’ let’s pierce the veil. Let’s become aware of the actual techniques that are being used. In the 90s and 00s, regular software development was in the position machine learning is in now. It all sounded vaguely magical if you weren’t a software developer. But over time that’s changed. Product teams and business analysts can more carefully reason about a broad range software techniques. Should an app be built to run entirely client-side in a browser, run in Javascript? Or should it be something built out entirely server-side? Is this a relational database? Or something else? Your average BA might not be able to get deep in the technical weeds, but many know enough to think critically and push back on how these technical decisions impact product needs. We need to get there with machine learning and search relevance. \\n\\n Everyone needs to be a scientist, not just data scientists \\n\\n To become a ‘relevance centered’ enterprise, the hardest adjustment I’ve seen in organizations is moving away from a traditional software development mindset to one that’s more scientific and hypothesis-driven. The idea that 93% of experiments will fail is hard to fathom. All that investment, to get that little nugget of success? The reality is, that’s just honest science! In a mature,  hypothesis-driven environment negative results and failures are expected. The goal is to improve the pace and quality of experimentation, where value is gained as much by knowledge gained not just movement of metrics. Fail fast, succeed with confidence. \\n\\n Changing how our organizations work on software is challenging. So understandably, organizations want to outsource the problem to a ‘cognitive search’ vendor. Organizations, managers, and executives often think ‘my vendor can handle this headache for me’. But really this is just punting organizational issues down the road. If search is core business value, eventually you’ll lose to competition that can act with more scientific agility to attack problems in the marketplace. When you can quickly experiment with real solutions in the marketplace, without disruptive organizational bureaucracy or vendor dependencies, you gain an incredible advantage that’s more fundamental than any machine learning technique. So think carefully about how your team, vendors, and the broader organization fits together to create a fast, experimental pace. \\n\\n We need Machine Learning literacy beyond the Data Science team \\n\\n The underlying problem behind buzzwords like ‘cognitive search’ is the lack of machine learning literacy outside of data science teams. A common problem when we consult with search teams is seeing a data science team developing capabilities which the rest of the search team ends up not using. Largely because most of the search team doesn’t know what to do with them. To the search team, the delivered ML model is a black box, impossible to reason about or manage, and usually developed in a siloed data science team without a tremendous amount of external input. Understandably, the data science team is equally frustrated: why aren’t they using this promising new technique? This dynamic really needs to change. \\n\\n I’m pleased to begin to see job titles like “Machine Learning Engineer”, tasked to be aware of machine learning sufficiently to think critically about how machine learning works in production. Our conception of ‘Relevance Engineer’ is similar: someone aware enough to use and develop machine learning information retrieval capabilities in production, even if they’re not going to innovate the fundamental science. For example, as engineers, we can all appreciate and learn how LambdaMART works but we don’t all need to be creating brand new learning to rank methods. \\n\\n These are just initial baby steps, we can’t make progress until everyone has awareness and education of machine learning concerns. In the same way product teams can now think critically about many aspects of software development, so must they now think about how the extremely diverse and large universe of machine learning techniques fits in. It’s not just one thing called ‘cognitive search’ but a huge universe of techniques with tradeoffs and areas for your organization to innovate and grow. \\n\\n That’s not all folks - \\n\\n I hope this wasn’t too much of a diatribe! Let me know if you disagree with my thoughts here, eager to hear other viewpoints! \\n\\n If you do take machine learning literacy seriously, please check out our Summer of Relevance - 4 days of training where we go from basic relevance, measuring search quality, up to advanced machine learning techniques that you can’ implement on your own. As always get in touch if we can be a part of increasing your orgs maturity with search and relevance through consulting or training. \\n',\n",
       " 'excerpt': ' We won’t address machine learning illiteracy in search if we can’t go beyond buzzwords. We need to teach concrete, specific techniques. \\n',\n",
       " 'post_date': '2019-05-28T00:00:00-0400',\n",
       " 'categories': ['blog', 'Relevancy', 'Learning-to-rank'],\n",
       " 'id': 2883614620}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "articles = []\n",
    "\n",
    "with open('data/blog.jsonl') as f:\n",
    "    for line in f:\n",
    "        blog = json.loads(line)\n",
    "        articles.append(blog)\n",
    "\n",
    "articles[-7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate an Elasticsearch client (as opposed to a `SolrClient`). Hello LTR can work with either "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.client import ElasticClient\n",
    "client=ElasticClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reindex from the corpus into the `blog` index. The JSON file at `docker/elasticsearch/<index_name>_settings.json` is loaded to configure the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created index blog [Status: 200]\n",
      "Streaming Bulk index DONE blog [Status: 201]\n"
     ]
    }
   ],
   "source": [
    "from ltr.index import rebuild\n",
    "rebuild(client, index='blog', doc_src=articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A set of features that we've come up with that seems to work well for OSC's blog. Note here, these are Elasticsearch specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed Default LTR feature store [Status: 404]\n",
      "{\"error\":{\"root_cause\":[{\"type\":\"index_not_found_exception\",\"reason\":\"no such index [.ltrstore]\",\"resource.type\":\"index_or_alias\",\"resource.id\":\".ltrstore\",\"index_uuid\":\"_na_\",\"index\":\".ltrstore\"}],\"type\":\"index_not_found_exception\",\"reason\":\"no such index [.ltrstore]\",\"resource.type\":\"index_or_alias\",\"resource.id\":\".ltrstore\",\"index_uuid\":\"_na_\",\"index\":\".ltrstore\"},\"status\":404}\n",
      "Initialize Default LTR feature store [Status: 200]\n",
      "Create test feature set [Status: 201]\n"
     ]
    }
   ],
   "source": [
    "client.reset_ltr(index='tmdb')\n",
    "\n",
    "config = {\n",
    "    \"featureset\": {\n",
    "        \"features\": [\n",
    "            {\n",
    "                \"name\": \"title_term_match\",\n",
    "                \"params\": [\"keywords\"],\n",
    "                \"template\": {\n",
    "                    \"constant_score\": {\n",
    "                       \"filter\": {\n",
    "                            \"match\": {\n",
    "                                \"title\": \"{{keywords}}\"\n",
    "                            }\n",
    "                       },\n",
    "                       \"boost\": 1.0\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "           {\n",
    "                \"name\": \"content_bm25\",\n",
    "                \"params\": [\"keywords\"],\n",
    "                \"template\": {\n",
    "                    \"match\": {\n",
    "                       \"content\": {\n",
    "                          \"query\": \"{{keywords}}\"\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"title_phrase_bm25\",\n",
    "                \"params\": [\"keywords\"],\n",
    "                \"template\": {\n",
    "                    \"match_phrase\": {\n",
    "                       \"title\": \"{{ keywords }}\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"title_phrase_match\",\n",
    "                \"params\": [\"keywords\"],\n",
    "                \"template\": {\n",
    "                    \"constant_score\": {\n",
    "                       \"filter\": {\n",
    "                            \"match_phrase\": {\n",
    "                                \"title\": \"{{keywords}}\"\n",
    "                            }\n",
    "                       },\n",
    "                       \"boost\": 1.0\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \n",
    "            {\n",
    "                \"name\": \"stepwise_post_date\",\n",
    "                \"params\": [\"keywords\"],\n",
    "                \"template\": {\n",
    "                  \"function_score\": {\n",
    "                     \"query\": {\n",
    "                        \"match_all\": {\n",
    "                        }\n",
    "                     },\n",
    "                     \"boost_mode\": \"replace\",\n",
    "                     \"score_mode\": \"sum\",\n",
    "                     \"functions\": [\n",
    "                        {\n",
    "                            \"filter\": {\n",
    "                                \"range\": {\n",
    "                                    \"post_date\": {\n",
    "                                        \"gte\": \"now-180d\"\n",
    "                                    }\n",
    "                                }\n",
    "                            },\n",
    "                            \"weight\": \"100\"               \n",
    "                        },\n",
    "                        {\n",
    "                            \"filter\": {\n",
    "                                \"range\": {\n",
    "                                    \"post_date\": {\n",
    "                                        \"gte\": \"now-360d\"\n",
    "                                    }\n",
    "                                }\n",
    "                            },\n",
    "                            \"weight\": \"100\"               \n",
    "                        },\n",
    "                          {\n",
    "                            \"filter\": {\n",
    "                                \"range\": {\n",
    "                                    \"post_date\": {\n",
    "                                        \"gte\": \"now-90d\"\n",
    "                                    }\n",
    "                                }\n",
    "                            },\n",
    "                            \"weight\": \"100\"               \n",
    "                        }\n",
    "\n",
    "                     ]\n",
    "                  }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"category_phrase_bm25\",\n",
    "                \"params\": [\"keywords\"],\n",
    "                \"template\": {\n",
    "                    \"match_phrase\": {\n",
    "                       \"categories\": \"{{ keywords }}\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"excerpt_bm25\",\n",
    "                \"params\": [\"keywords\"],\n",
    "                \"template\": {\n",
    "                    \"match\": {\n",
    "                       \"excerpt\": \"{{ keywords }}\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"excerpt_phrase_bm25\",\n",
    "                \"params\": [\"keywords\"],\n",
    "                \"template\": {\n",
    "                    \"match_phrase\": {\n",
    "                       \"excerpt\": \"{{ keywords }}\"\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "        ]\n",
    "    },\n",
    "    \"validation\": {\n",
    "      \"index\": \"blog\",\n",
    "      \"params\": {\n",
    "          \"keywords\": \"rambo\"\n",
    "      }\n",
    "\n",
    "   }\n",
    "}\n",
    "\n",
    "client.create_featureset(index='blog', name='test', ftr_config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With features loaded, transform the judgment list (`query,doc,label`) into a full training set with `query,doc,label,ftr1,ftr2,...` to prepare for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognizing 47 queries...\n"
     ]
    }
   ],
   "source": [
    "from ltr.judgments import judgments_open\n",
    "from ltr.log import FeatureLogger\n",
    "from itertools import groupby\n",
    "\n",
    "ftr_logger=FeatureLogger(client, index='blog', feature_set='test')\n",
    "with judgments_open('data/osc_judgments.txt') as judgment_list:\n",
    "    for qid, query_judgments in groupby(judgment_list, key=lambda j: j.qid):\n",
    "        ftr_logger.log_for_qid(judgments=query_judgments, \n",
    "                               qid=qid,\n",
    "                               keywords=judgment_list.keywords(qid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train using RankyMcRankFace with the training set, optimizing search for a specific metric (here `NDCG@10`). Note `ltr.train` has additional capabilities for performing k-fold cross validaiton to ensure the model isn't overfit to training data.\n",
    "\n",
    "The model is stored in the search engine named `test` which can be referred to later for searching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/folders/7_/cvjz84n54vx7zv_pw3gmdqr00000gn/T/RankyMcRankFace.jar already exists\n",
      "Running java -jar /var/folders/7_/cvjz84n54vx7zv_pw3gmdqr00000gn/T/RankyMcRankFace.jar -ranker 6 -shrinkage 0.1 -metric2t NDCG@10 -tree 50 -bag 1 -leaf 10 -frate 1.0 -srate 1.0 -train /var/folders/7_/cvjz84n54vx7zv_pw3gmdqr00000gn/T/training.txt -save data/test_model.txt \n",
      "Delete model test: 404\n",
      "Created Model test [Status: 201]\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "from ltr.ranklib import train\n",
    "trainLog = train(client,\n",
    "                 training_set=ftr_logger.logged,\n",
    "                 metric2t='NDCG@10',\n",
    "                 featureSet='test',\n",
    "                 index='blog',\n",
    "                 modelName='test')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Discard orig. features\n",
      "Training data:\t/var/folders/7_/cvjz84n54vx7zv_pw3gmdqr00000gn/T/training.txt\n",
      "Feature vector representation: Dense.\n",
      "Ranking method:\tLambdaMART\n",
      "Feature description file:\tUnspecified. All features will be used.\n",
      "Train metric:\tNDCG@10\n",
      "Test metric:\tNDCG@10\n",
      "Feature normalization: No\n",
      "\n",
      "[+] LambdaMART's Parameters:\n",
      "No. of trees: 50\n",
      "No. of leaves: 10\n",
      "No. of threshold candidates: 256\n",
      "Min leaf support: 1\n",
      "Learning rate: 0.1\n",
      "Stop early: 100 rounds without performance gain on validation data\n",
      "\n",
      "Reading feature file [/var/folders/7_/cvjz84n54vx7zv_pw3gmdqr00000gn/T/training.txt]... [Done.]            \n",
      "(47 ranked lists, 477 entries read)\n",
      "Initializing... [Done]\n",
      "---------------------------------\n",
      "Training starts...\n",
      "---------------------------------\n",
      "#iter   | NDCG@10-T | NDCG@10-V | \n",
      "---------------------------------\n",
      "1       | 0.9393    | \n",
      "2       | 0.9367    | \n",
      "3       | 0.9367    | \n",
      "4       | 0.9367    | \n",
      "5       | 0.937     | \n",
      "6       | 0.9401    | \n",
      "7       | 0.9414    | \n",
      "8       | 0.9413    | \n",
      "9       | 0.9473    | \n",
      "10      | 0.9462    | \n",
      "11      | 0.9456    | \n",
      "12      | 0.9456    | \n",
      "13      | 0.9426    | \n",
      "14      | 0.9459    | \n",
      "15      | 0.9449    | \n",
      "16      | 0.945     | \n",
      "17      | 0.9448    | \n",
      "18      | 0.954     | \n",
      "19      | 0.954     | \n",
      "20      | 0.9537    | \n",
      "21      | 0.9532    | \n",
      "22      | 0.9532    | \n",
      "23      | 0.9595    | \n",
      "24      | 0.9629    | \n",
      "25      | 0.9629    | \n",
      "26      | 0.9628    | \n",
      "27      | 0.9639    | \n",
      "28      | 0.963     | \n",
      "29      | 0.9639    | \n",
      "30      | 0.9644    | \n",
      "31      | 0.9645    | \n",
      "32      | 0.9649    | \n",
      "33      | 0.9648    | \n",
      "34      | 0.9656    | \n",
      "35      | 0.9682    | \n",
      "36      | 0.9692    | \n",
      "37      | 0.9695    | \n",
      "38      | 0.9695    | \n",
      "39      | 0.9696    | \n",
      "40      | 0.9706    | \n",
      "41      | 0.9705    | \n",
      "42      | 0.9709    | \n",
      "43      | 0.971     | \n",
      "44      | 0.9709    | \n",
      "45      | 0.9716    | \n",
      "46      | 0.9717    | \n",
      "47      | 0.9712    | \n",
      "48      | 0.9715    | \n",
      "49      | 0.9715    | \n",
      "50      | 0.9715    | \n",
      "---------------------------------\n",
      "Finished sucessfully.\n",
      "NDCG@10 on training data: 0.9715\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "-- FEATURE IMPACTS\n",
      " Feature 3 reduced error 8.243334359161206\n",
      " Feature 2 reduced error 5.514713495184286\n",
      " Feature 8 reduced error 0.024945083590455153\n",
      " Feature 7 reduced error 0.01878413667461713\n",
      " Feature 1 reduced error 0.005004028611134755\n",
      " Feature 6 reduced error 5.735968244073788E-6\n",
      " Feature 4 reduced error 0.0\n",
      " Feature 5 reduced error 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!java -jar /var/folders/7_/cvjz84n54vx7zv_pw3gmdqr00000gn/T/RankyMcRankFace.jar -ranker 6 -shrinkage 0.1 -metric2t NDCG@10 -tree 50 -bag 1 -leaf 10 -frate 1.0 -srate 1.0 -train /var/folders/7_/cvjz84n54vx7zv_pw3gmdqr00000gn/T/training.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search! Pass some configuration in (`blog_fields`) for display purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_fields = {\n",
    "    'title': 'title',\n",
    "    'display_fields': ['url', 'author', 'categories', 'post_date']\n",
    "}\n",
    "\n",
    "from ltr import search\n",
    "search(client, \"beer\", modelName='test', \n",
    "       index='blog', fields=blog_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
